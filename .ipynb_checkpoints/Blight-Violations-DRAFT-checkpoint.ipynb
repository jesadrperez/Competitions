{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Blight Violations\n",
    "\n",
    "_This project was completed as a part of the Applied Data Science with Python Specialization from Coursera._\n",
    "\n",
    "In this project, we will evaluate the performance and predictive power of a model that has been trained and tested on data collected from Blight Violation Notices (BVN), or Blight Tickets, that have been issued to property owners who have violated City of Detroit ordinances that govern how property owners must maintain the exterior of their property. Blight Tickets are issued by city inspectors, police officers, neighborhood city hall managers and other city officials who investigate complaints of blight and are managed by the Department of Administrative Hearings.\n",
    "\n",
    "The target variable is compliance, which is __True__ if the ticket was paid early, on time, or within one month of the hearing data, __False__ if the ticket was paid after the hearing date or not at all, and __Null__ if the violator was found not responsible. Compliance is not avaliable in the dataset so it must be calculated.\n",
    "\n",
    "The dataset for this project originates from the City of Detroit's Open Data Portal initiative and is updated daily. For this project the dataset spans tickets issued from March 2004 to March 2018. The dataset is avaliable at: https://data.detroitmi.gov/Property-Parcels/Blight-Violations/ti6p-wcg4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing\n",
    "\n",
    "Imports the raw data, calculates compliance for each ticket, removes features that cause data leakage, and seperates the data into two dataset based on when the violation occured. Violations issused before 2017 will be used to train and test the model, and tickets issued on and after 2017 will be used to validate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import normalize\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "# To make the notebook's output stable across runs\n",
    "random_seed = 12062017\n",
    "np.random.seed(random_seed)\n",
    "\n",
    "matplotlib.style.use('ggplot') \n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import sparse\n",
    "\n",
    "class CategoricalEncoder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Encode categorical features as a numeric array.\n",
    "    The input to this transformer should be a matrix of integers or strings,\n",
    "    denoting the values taken on by categorical (discrete) features.\n",
    "    The features can be encoded using a one-hot aka one-of-K scheme\n",
    "    (``encoding='onehot'``, the default) or converted to ordinal integers\n",
    "    (``encoding='ordinal'``).\n",
    "    This encoding is needed for feeding categorical data to many scikit-learn\n",
    "    estimators, notably linear models and SVMs with the standard kernels.\n",
    "    Read more in the :ref:`User Guide <preprocessing_categorical_features>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    encoding : str, 'onehot', 'onehot-dense' or 'ordinal'\n",
    "        The type of encoding to use (default is 'onehot'):\n",
    "        - 'onehot': encode the features using a one-hot aka one-of-K scheme\n",
    "          (or also called 'dummy' encoding). This creates a binary column for\n",
    "          each category and returns a sparse matrix.\n",
    "        - 'onehot-dense': the same as 'onehot' but returns a dense array\n",
    "          instead of a sparse matrix.\n",
    "        - 'ordinal': encode the features as ordinal integers. This results in\n",
    "          a single column of integers (0 to n_categories - 1) per feature.\n",
    "    categories : 'auto' or a list of lists/arrays of values.\n",
    "        Categories (unique values) per feature:\n",
    "        - 'auto' : Determine categories automatically from the training data.\n",
    "        - list : ``categories[i]`` holds the categories expected in the ith\n",
    "          column. The passed categories are sorted before encoding the data\n",
    "          (used categories can be found in the ``categories_`` attribute).\n",
    "    dtype : number type, default np.float64\n",
    "        Desired dtype of output.\n",
    "    handle_unknown : 'error' (default) or 'ignore'\n",
    "        Whether to raise an error or ignore if a unknown categorical feature is\n",
    "        present during transform (default is to raise). When this is parameter\n",
    "        is set to 'ignore' and an unknown category is encountered during\n",
    "        transform, the resulting one-hot encoded columns for this feature\n",
    "        will be all zeros.\n",
    "        Ignoring unknown categories is not supported for\n",
    "        ``encoding='ordinal'``.\n",
    "    Attributes\n",
    "    ----------\n",
    "    categories_ : list of arrays\n",
    "        The categories of each feature determined during fitting. When\n",
    "        categories were specified manually, this holds the sorted categories\n",
    "        (in order corresponding with output of `transform`).\n",
    "    Examples\n",
    "    --------\n",
    "    Given a dataset with three features and two samples, we let the encoder\n",
    "    find the maximum value per feature and transform the data to a binary\n",
    "    one-hot encoding.\n",
    "    >>> from sklearn.preprocessing import CategoricalEncoder\n",
    "    >>> enc = CategoricalEncoder(handle_unknown='ignore')\n",
    "    >>> enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])\n",
    "    ... # doctest: +ELLIPSIS\n",
    "    CategoricalEncoder(categories='auto', dtype=<... 'numpy.float64'>,\n",
    "              encoding='onehot', handle_unknown='ignore')\n",
    "    >>> enc.transform([[0, 1, 1], [1, 0, 4]]).toarray()\n",
    "    array([[ 1.,  0.,  0.,  1.,  0.,  0.,  1.,  0.,  0.],\n",
    "           [ 0.,  1.,  1.,  0.,  0.,  0.,  0.,  0.,  0.]])\n",
    "    See also\n",
    "    --------\n",
    "    sklearn.preprocessing.OneHotEncoder : performs a one-hot encoding of\n",
    "      integer ordinal features. The ``OneHotEncoder assumes`` that input\n",
    "      features take on values in the range ``[0, max(feature)]`` instead of\n",
    "      using the unique values.\n",
    "    sklearn.feature_extraction.DictVectorizer : performs a one-hot encoding of\n",
    "      dictionary items (also handles string-valued features).\n",
    "    sklearn.feature_extraction.FeatureHasher : performs an approximate one-hot\n",
    "      encoding of dictionary items or strings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoding='onehot', categories='auto', dtype=np.float64,\n",
    "                 handle_unknown='error'):\n",
    "        self.encoding = encoding\n",
    "        self.categories = categories\n",
    "        self.dtype = dtype\n",
    "        self.handle_unknown = handle_unknown\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit the CategoricalEncoder to X.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_feature]\n",
    "            The data to determine the categories of each feature.\n",
    "        Returns\n",
    "        -------\n",
    "        self\n",
    "        \"\"\"\n",
    "\n",
    "        if self.encoding not in ['onehot', 'onehot-dense', 'ordinal']:\n",
    "            template = (\"encoding should be either 'onehot', 'onehot-dense' \"\n",
    "                        \"or 'ordinal', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.handle_unknown not in ['error', 'ignore']:\n",
    "            template = (\"handle_unknown should be either 'error' or \"\n",
    "                        \"'ignore', got %s\")\n",
    "            raise ValueError(template % self.handle_unknown)\n",
    "\n",
    "        if self.encoding == 'ordinal' and self.handle_unknown == 'ignore':\n",
    "            raise ValueError(\"handle_unknown='ignore' is not supported for\"\n",
    "                             \" encoding='ordinal'\")\n",
    "\n",
    "        X = check_array(X, dtype=np.object, accept_sparse='csc', copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        self._label_encoders_ = [LabelEncoder() for _ in range(n_features)]\n",
    "\n",
    "        for i in range(n_features):\n",
    "            le = self._label_encoders_[i]\n",
    "            Xi = X[:, i]\n",
    "            if self.categories == 'auto':\n",
    "                le.fit(Xi)\n",
    "            else:\n",
    "                valid_mask = np.in1d(Xi, self.categories[i])\n",
    "                if not np.all(valid_mask):\n",
    "                    if self.handle_unknown == 'error':\n",
    "                        diff = np.unique(Xi[~valid_mask])\n",
    "                        msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                               \" during fit\".format(diff, i))\n",
    "                        raise ValueError(msg)\n",
    "                le.classes_ = np.array(np.sort(self.categories[i]))\n",
    "\n",
    "        self.categories_ = [le.classes_ for le in self._label_encoders_]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\"Transform X using one-hot encoding.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like, shape [n_samples, n_features]\n",
    "            The data to encode.\n",
    "        Returns\n",
    "        -------\n",
    "        X_out : sparse matrix or a 2-d array\n",
    "            Transformed input.\n",
    "        \"\"\"\n",
    "        X = check_array(X, accept_sparse='csc', dtype=np.object, copy=True)\n",
    "        n_samples, n_features = X.shape\n",
    "        X_int = np.zeros_like(X, dtype=np.int)\n",
    "        X_mask = np.ones_like(X, dtype=np.bool)\n",
    "\n",
    "        for i in range(n_features):\n",
    "            valid_mask = np.in1d(X[:, i], self.categories_[i])\n",
    "\n",
    "            if not np.all(valid_mask):\n",
    "                if self.handle_unknown == 'error':\n",
    "                    diff = np.unique(X[~valid_mask, i])\n",
    "                    msg = (\"Found unknown categories {0} in column {1}\"\n",
    "                           \" during transform\".format(diff, i))\n",
    "                    raise ValueError(msg)\n",
    "                else:\n",
    "                    # Set the problematic rows to an acceptable value and\n",
    "                    # continue `The rows are marked `X_mask` and will be\n",
    "                    # removed later.\n",
    "                    X_mask[:, i] = valid_mask\n",
    "                    X[:, i][~valid_mask] = self.categories_[i][0]\n",
    "            X_int[:, i] = self._label_encoders_[i].transform(X[:, i])\n",
    "\n",
    "        if self.encoding == 'ordinal':\n",
    "            return X_int.astype(self.dtype, copy=False)\n",
    "\n",
    "        mask = X_mask.ravel()\n",
    "        n_values = [cats.shape[0] for cats in self.categories_]\n",
    "        n_values = np.array([0] + n_values)\n",
    "        indices = np.cumsum(n_values)\n",
    "\n",
    "        column_indices = (X_int + indices[:-1]).ravel()[mask]\n",
    "        row_indices = np.repeat(np.arange(n_samples, dtype=np.int32),\n",
    "                                n_features)[mask]\n",
    "        data = np.ones(n_samples * n_features)[mask]\n",
    "\n",
    "        out = sparse.csc_matrix((data, (row_indices, column_indices)),\n",
    "                                shape=(n_samples, indices[-1]),\n",
    "                                dtype=self.dtype).tocsr()\n",
    "        if self.encoding == 'onehot-dense':\n",
    "            return out.toarray()\n",
    "        else:\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_blight_raw(blight_path):\n",
    "    '''\n",
    "    Loads the raw blight_violations.csv\n",
    "    Returns a pandas dataframe.\n",
    "    '''\n",
    "    csv_path = os.path.join(blight_path, \"Blight_Violations.csv\")\n",
    "    blight_raw = pd.read_csv(csv_path, encoding='ISO-8859-1')\n",
    "    print(\"RAW Blight Violations dataset has {} observations with {} features each.\".format(*blight_raw.shape))\n",
    "    return blight_raw\n",
    "\n",
    "def pre_processing(blight_df):\n",
    "    ''' \n",
    "    Takes blight panda dataframe and makes column names more reabable, sets ticket_id as index, creates compliance (the \n",
    "    prediction variable), and creates compliance_details (explains why a tickets was labeled as complantent or non-complantent).\n",
    "    Returns a panda dataframe.\n",
    "    '''\n",
    "    # Makes column names more reabable\n",
    "    # Gets columns from dataframe\n",
    "    columns_names = pd.Series(blight_df.columns)\n",
    "    # Removes parentheses, removes right spaces\n",
    "    columns_names = columns_names.str.split('(').str[0].str.strip()\n",
    "    # Coverts column names to lowercase and replaces spaces with underscores\n",
    "    columns_names = columns_names.str.lower().str.replace(' ', '_')\n",
    "    blight_df.columns = columns_names\n",
    "    # Sets ticket_id as index\n",
    "    blight_df.set_index('ticket_id', inplace=True)\n",
    "    # Defines prediction variable compliance and helper variable compliance_detail\n",
    "    # The values of these variables are NOT correct\n",
    "    blight_df['compliance'] = 0\n",
    "    blight_df['compliance_detail'] = np.NaN    \n",
    "    return blight_df\n",
    "\n",
    "def clean_up(blight_df):\n",
    "    # Removes instances where 'ticket_issued_date' is not in (2000, 2017) \n",
    "    blight_df = blight_df[blight_df['violation_date'].str.contains('[0-9]{2}/[0-9]{2}/[20]{2}[0-9]{2}') == True]\n",
    "    # Converts 'ticket_issued_date' to datetime\n",
    "    blight_df.loc[:, 'violation_date'] = pd.to_datetime(blight_df['violation_date'])\n",
    "    # Converts 'payment_date' to datetime\n",
    "    blight_df.loc[:, 'payment_date'] = pd.to_datetime(blight_df['payment_date'].str.extract('([0-9]{2}/[0-9]{2}/20{1}[0-9]{2})')) \n",
    "    # Converts bad values in 'ticket_issued_time' to NaNs (Dataset error removed by publisher)\n",
    "    #blight_df.loc[blight_df['violation_date'] == '00000000000000.000', 'violation_date'] = np.nan\n",
    "    # Converts 'ticket_issued_time' to datetime.time\n",
    "    blight_df.loc[:, 'ticket_issued_time'] = pd.DatetimeIndex(blight_df['ticket_issued_time']).time\n",
    "    # Converts 'hearing_time' to datetime.time\n",
    "    blight_df.loc[:, 'hearing_time'] = pd.DatetimeIndex(blight_df['hearing_time']).time\n",
    "    # Remove \"$\" from 'judgement_amount'\n",
    "    blight_df.loc[:, 'judgment_amount'] = blight_df['judgment_amount'].str.strip('$')    \n",
    "    # Converts 'judgment_amount' to float    \n",
    "    blight_df.loc[:, 'judgment_amount'] = blight_df['judgment_amount'].astype(float)\n",
    "    # Removes \"$\" from 'payment_amount' and converts it to float\n",
    "    blight_df.loc[:, 'payment_amount'] = blight_df['payment_amount'].str.strip('$').astype(float)\n",
    "    # Removes \"$\" from 'fine_amount' and converts it to float\n",
    "    blight_df.loc[:, 'fine_amount'] = blight_df['fine_amount'].str.strip('$').astype(float)\n",
    "    # Removes \"$\" from 'admin_fee' and converts it to float\n",
    "    blight_df.loc[:, 'admin_fee'] = blight_df['admin_fee'].str.strip('$').astype(float)\n",
    "    # Removes \"$\" from 'state_fee' and converts it to float\n",
    "    blight_df.loc[:, 'state_fee'] = blight_df['state_fee'].str.strip('$').astype(float)\n",
    "    # Removes \"$\" from 'late_fee' and converts it to float\n",
    "    blight_df.loc[:, 'late_fee']   = blight_df['late_fee'].str.strip('$').astype(float)\n",
    "    # Removes \"$\" from 'discount_amount' and converts it to float\n",
    "    blight_df.loc[:, 'discount_amount'] = blight_df['discount_amount'].str.strip('$').astype(float)    \n",
    "    return blight_df\n",
    "\n",
    "def null_compliance(blight_df):\n",
    "    '''\n",
    "    Compliance = np.NaN\n",
    "    Tickets that cannot be classified as compliant or non-compliant because they were ruled as not responsible in disposition\n",
    "    or they are awaiting judgement.\n",
    "    Returns a pandas dataframe\n",
    "    '''\n",
    "    # Dispositions that are ruled as not responsible or are still pending\n",
    "    null_dispositions = ['Not responsible by Dismissal', 'Not responsible by City Dismissal', 'PENDING JUDGMENT', \n",
    "                 'Not responsible by Determination','SET-ASIDE (PENDING JUDGMENT)', 'PENDING', 'Responsible by Dismissal']\n",
    "    # Loops over dispositions and sets 'compliance' values to np.NaN\n",
    "    # Loops over dispositions and sets 'compliance_detail' values to 'Not Responsible/Pending Judgement'\n",
    "    for dispositions in null_dispositions:\n",
    "        blight_df.loc[blight_df['disposition'] == dispositions, 'compliance'] = np.NaN\n",
    "        blight_df.loc[blight_df['disposition'] == dispositions, 'compliance_detail'] = 'Not Responsible/Pending Judgement'\n",
    "    return blight_df\n",
    "\n",
    "def compliant(blight_df):\n",
    "    '''\n",
    "    Compliance = 1\n",
    "    Tickets that are classified as compliant because they had no fine, fine was waved, made a payment with hearing pending,\n",
    "    early payment (hearing not pending), payment on time, or payment within one month after hearing date.\n",
    "    Returns a pandas dataframe\n",
    "    '''\n",
    "    ## Compliant by no fine\n",
    "    # Fine Waived by Determintation\n",
    "    blight_df.loc[blight_df['disposition'] == 'Responsible (Fine Waived) by Determination', 'compliance'] = 1\n",
    "    blight_df.loc[blight_df['disposition'] == 'Responsible (Fine Waived) by Determination', 'compliance_detail'] = 'Compliant by no fine'\n",
    "    # Fine Waived by Admission\n",
    "    blight_df.loc[blight_df['disposition'] == 'Responsible (Fine Waived) by Admission', 'compliance'] = 1\n",
    "    blight_df.loc[blight_df['disposition'] == 'Responsible (Fine Waived) by Admission', 'compliance_detail'] = 'Compliant by no fine'\n",
    "    ## Compliant by Payment\n",
    "    # Payment with PENDING hearing\n",
    "    blight_df.loc[(\n",
    "        (blight_df['hearing_date'] == 'PENDING') &\n",
    "        (blight_df['payment_amount'] > 0) &\n",
    "        (blight_df['compliance_detail'].isnull())), 'compliance'] = 1\n",
    "    blight_df.loc[(\n",
    "        (blight_df['hearing_date'] == 'PENDING') &\n",
    "        (blight_df['payment_amount'] > 0) &\n",
    "        (blight_df['compliance_detail'].isnull())), 'compliance_detail'] = 'Compliant by payment with PENDING hearing'\n",
    "    # Transforms values in 'hearing_date' and 'payment_date' to panda date types\n",
    "    dummy_hearing_date = blight_df['hearing_date'].copy()\n",
    "    blight_df.loc[blight_df['hearing_date'] == 'PENDING', 'hearing_date'] = np.nan\n",
    "    blight_df.loc[:, 'hearing_date'] = pd.to_datetime(blight_df['hearing_date'].str.extract('([0-9]{2}/[0-9]{2}/20{1}[0-9]{2})'))\n",
    "    # Early Payment, payment before hearing date\n",
    "    blight_df.loc[(\n",
    "        (blight_df['payment_date'] < blight_df['hearing_date']) &\n",
    "        (blight_df['payment_amount'] > 0) &\n",
    "        (blight_df['compliance_detail'].isnull())), 'compliance'] = 1\n",
    "    blight_df.loc[(\n",
    "        (blight_df['payment_date'] < blight_df['hearing_date']) &\n",
    "        (blight_df['payment_amount'] > 0) &\n",
    "        (blight_df['compliance_detail'].isnull())), 'compliance_detail'] = 'Compliant by early payment'\n",
    "    # Payment on time, payment on hearing date\n",
    "    blight_df.loc[(\n",
    "        (blight_df['payment_date'] == blight_df['hearing_date']) &\n",
    "        (blight_df['payment_amount'] > 0) &\n",
    "        (blight_df['compliance_detail'].isnull())), 'compliance'] = 1\n",
    "    blight_df.loc[(\n",
    "        (blight_df['payment_date'] == blight_df['hearing_date']) &\n",
    "        (blight_df['payment_amount'] > 0) &\n",
    "        (blight_df['compliance_detail'].isnull())), 'compliance_detail'] = 'Compliant by payment on time'\n",
    "    # Payment within one month after hearing date\n",
    "    blight_df.loc[(\n",
    "        ((blight_df['payment_date'] - blight_df['hearing_date'])/np.timedelta64(1, 'M') <= 1.0) &\n",
    "        (blight_df['payment_amount'] > 0) &\n",
    "        (blight_df['compliance_detail'].isnull())), 'compliance'] = 1\n",
    "    blight_df.loc[(\n",
    "        ((blight_df['payment_date'] - blight_df['hearing_date'])/np.timedelta64(1, 'M') <= 1.0) &\n",
    "        (blight_df['payment_amount'] > 0) &\n",
    "        (blight_df['compliance_detail'].isnull())), 'compliance_detail'] = 'Compliant by payment within 1 Month'\n",
    "    # Sets 'hearing_date' to orginal state\n",
    "    blight_df.loc[:, 'hearing_date'] = dummy_hearing_date\n",
    "    return blight_df\n",
    "\n",
    "def non_compliant(blight_df):\n",
    "    '''\n",
    "    Compliance = 0\n",
    "    Tickets that are classified as non-compliant because they made no payment or a payment after one month (late payment).\n",
    "    Returns a pandas dataframe\n",
    "    '''\n",
    "    # Transforms values in 'hearing_date' to panda date types\n",
    "    dummy_hearing_date = blight_df['hearing_date'].copy()\n",
    "    blight_df.loc[blight_df['hearing_date'] == 'PENDING', 'hearing_date'] = np.nan\n",
    "    blight_df.loc[:, 'hearing_date'] = pd.to_datetime(blight_df['hearing_date'].str.extract('([0-9]{2}/[0-9]{2}/20{1}[0-9]{2})'))\n",
    "    # Non-compliant by late payment\n",
    "    blight_df.loc[(\n",
    "        ((blight_df['payment_date'] - blight_df['hearing_date'])/np.timedelta64(1, 'M') > 1.0) &\n",
    "        (blight_df['payment_amount'] > 0) &\n",
    "        (blight_df['compliance_detail'].isnull())), 'compliance_detail'] = 'Non-compliant by late payment more than 1 month'\n",
    "    # Non-compliant by no payment\n",
    "    blight_df.loc[blight_df['compliance_detail'].isnull(), 'compliance_detail'] = 'Non-compliant by no payment'\n",
    "    # Sets 'hearing_date' to orginal state\n",
    "    blight_df.loc[:, 'hearing_date'] = dummy_hearing_date\n",
    "    return blight_df\n",
    "\n",
    "def populate_compliance(blight_df):\n",
    "    '''\n",
    "    Populates compliance and compliance_details in blight_df with the correct values. \n",
    "    Returns a panda dataframe.\n",
    "    '''\n",
    "    blight_df = null_compliance(blight_df)\n",
    "    blight_df = compliant(blight_df)\n",
    "    blight_df = non_compliant(blight_df)  \n",
    "    return blight_df\n",
    "\n",
    "def remove_leakage(blight_df):\n",
    "    ''' \n",
    "    In blight_df removes variables to prevent data leakage and variables with mostly NaNs,\n",
    "    Returns a panda dataframe\n",
    "    '''\n",
    "    blight_df = blight_df[['agency_name', 'inspector_name', 'violator_name','violation_street_number', 'violation_street_name', \n",
    "                       'mailing_address_street_name', 'mailing_address_street_name', 'mailing_address_city', \n",
    "                       'mailing_address_state', 'mailing_address_zip_code', 'mailing_address_non-usa_code', \n",
    "                       'mailing_address_country', 'violation_date', 'hearing_date', 'hearing_time', 'violation_code', 'violation_description',\n",
    "                       'disposition', 'fine_amount', 'admin_fee', 'state_fee', 'late_fee', 'discount_amount', \n",
    "                       'judgment_amount', 'violation_latitude', 'violation_longitude', 'compliance']]  \n",
    "    return blight_df \n",
    "\n",
    "def process_blight(blight_path, pre_process):\n",
    "    '''\n",
    "    If pre_process is true - Loads the raw blight_violations.csv, cleans the data, computes compliance, removes features with \n",
    "    data leakage and saves this dataframe. If pre_process is False, then it loads the pre-processed data.\n",
    "    Returns a pandas dataframe.\n",
    "    '''\n",
    "    if pre_process == False:\n",
    "        blight_df = load_blight_raw(blight_path)\n",
    "        blight_df = pre_processing(blight_df)\n",
    "        blight_df = clean_up(blight_df)\n",
    "        blight_df = populate_compliance(blight_df)\n",
    "        blight_df = remove_leakage(blight_df)\n",
    "        save_blight_data(blight_path, blight_df)\n",
    "    else:\n",
    "        blight_df = load_blight(blight_path)\n",
    "    print(\"PROCESSED Blight Violations dataset has {} observations with {} features each.\".format(*blight_df.shape))        \n",
    "    return blight_df\n",
    "\n",
    "def save_blight_data(blight_path, blight_df):\n",
    "    '''\n",
    "    Saves the processed blight_df\n",
    "    Returns nothing.\n",
    "    '''\n",
    "    csv_path = os.path.join(blight_path, \"Blight_Violations_Processed.csv\")\n",
    "    blight_df.to_csv(csv_path)\n",
    "    \n",
    "def load_blight(blight_path):\n",
    "    '''\n",
    "    Loads the preprocessed blight_violations.csv\n",
    "    Returns a pandas dataframe.\n",
    "    '''\n",
    "    csv_path = os.path.join(blight_path, \"Blight_Violations_Processed.csv\")\n",
    "    blight_df = pd.read_csv(csv_path)\n",
    "    blight_df.set_index('ticket_id', inplace=True)\n",
    "    return blight_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROCESSED Blight Violations dataset has 373844 observations with 27 features each.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 373844 entries, 47056 to 247933\n",
      "Data columns (total 27 columns):\n",
      "agency_name                      373844 non-null object\n",
      "inspector_name                   373844 non-null object\n",
      "violator_name                    373842 non-null object\n",
      "violation_street_number          373844 non-null int64\n",
      "violation_street_name            373779 non-null object\n",
      "mailing_address_street_name      373841 non-null object\n",
      "mailing_address_street_name.1    373841 non-null object\n",
      "mailing_address_city             372028 non-null object\n",
      "mailing_address_state            371565 non-null object\n",
      "mailing_address_zip_code         372025 non-null object\n",
      "mailing_address_non-usa_code     1819 non-null object\n",
      "mailing_address_country          1830 non-null object\n",
      "violation_date                   373844 non-null object\n",
      "hearing_date                     373234 non-null object\n",
      "hearing_time                     373233 non-null object\n",
      "violation_code                   373843 non-null object\n",
      "violation_description            373843 non-null object\n",
      "disposition                      365161 non-null object\n",
      "fine_amount                      373842 non-null float64\n",
      "admin_fee                        373844 non-null float64\n",
      "state_fee                        373844 non-null float64\n",
      "late_fee                         373844 non-null float64\n",
      "discount_amount                  373844 non-null float64\n",
      "judgment_amount                  373844 non-null float64\n",
      "violation_latitude               373719 non-null float64\n",
      "violation_longitude              373719 non-null float64\n",
      "compliance                       251359 non-null float64\n",
      "dtypes: float64(9), int64(1), object(17)\n",
      "memory usage: 79.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Loads the raw blight data and pre-processes\n",
    "#blight_df = process_blight(r\"C:\\Users\\Adrian\\Google Drive\\Datasets\\Blight-Violations\", False)\n",
    "blight_df = process_blight(r\"C:\\Users\\aperez\\Google Drive\\Datasets\\Blight-Violations\", True)\n",
    "# Checks to make sure the data loaded properly\n",
    "blight_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets remove unrelevant fields and useless observations -\n",
    "* Observations where compliance is _NaNs_, these observations provide no use to me\n",
    "* Features that are too noisey or contain many _NaNs_: inspector_name, violator_name, violation_street_number, violation_street_name, mailing_address_street_name, mailing_address_street_name.1, violation_code, and violation_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 242676 entries, 47056 to 359077\n",
      "Data columns (total 19 columns):\n",
      "agency_name                      242676 non-null object\n",
      "mailing_address_street_name.1    242673 non-null object\n",
      "mailing_address_city             241130 non-null object\n",
      "mailing_address_state            240715 non-null object\n",
      "mailing_address_country          1557 non-null object\n",
      "violation_date                   242676 non-null object\n",
      "hearing_date                     242676 non-null object\n",
      "hearing_time                     242676 non-null object\n",
      "violation_description            242675 non-null object\n",
      "disposition                      242676 non-null object\n",
      "fine_amount                      242675 non-null float64\n",
      "admin_fee                        242676 non-null float64\n",
      "state_fee                        242676 non-null float64\n",
      "late_fee                         242676 non-null float64\n",
      "discount_amount                  242676 non-null float64\n",
      "judgment_amount                  242676 non-null float64\n",
      "violation_latitude               242643 non-null float64\n",
      "violation_longitude              242643 non-null float64\n",
      "compliance                       242676 non-null float64\n",
      "dtypes: float64(9), object(10)\n",
      "memory usage: 37.0+ MB\n"
     ]
    }
   ],
   "source": [
    "# Drop observations where compliance is NaNs\n",
    "blight_df.dropna(axis=0, subset=['compliance'], inplace=True)\n",
    "# Remove obs with NaN in disposition\n",
    "blight_df.dropna(axis=0, subset=['disposition'], inplace=True)\n",
    "# Drop useless features\n",
    "blight_df.drop(axis=1, columns=['inspector_name', 'violator_name', 'violation_street_number', 'violation_street_name',\n",
    "       'mailing_address_street_name', 'mailing_address_zip_code', 'mailing_address_non-usa_code',\n",
    "        'violation_code'], inplace=True)\n",
    "blight_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering \n",
    "\n",
    "Now, we will construct new features. These are:\n",
    "* detriot_resident - _True_ if 'mailing_address_city' is Detriot, _False_ otherwise\n",
    "* michigan_resident - _True_ if 'mailing_address_state' is Michigan, _False_ otherwise\n",
    "* us_resident - _True_ if 'mailing_address_state' is not null but _False_ if 'mailing_address_country' is not null, _False_ otherwise\n",
    "* violation_month = the numeric month (1-12) that violation occured on\n",
    "* days_difference = the number of days between when the violation occured and when the hearing date is\n",
    "* hearing_weekday - name of the day of the week the hearing date is on. This features will later be categorically encoded into five features based on it values.\n",
    "* disposition_clean = a cleaned version  of \"disposition\" that can either be: 'Fine Waived', 'Default', 'Admission', or 'Determination'. This features will later be categorically encoded into four features.\n",
    "* agency_name - This features will later be categorically encoded into five features based on it values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### detriot_resident\n",
    "\n",
    "First fill \"NaN\" values with \"unknown\", then convert all the characters to lower case, and removes all punctuation. Finally use regular expressions to find all strings that start with \"det\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aperez\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "  \n",
      "C:\\Users\\aperez\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Creates 'detriot_resident' feature, setting all values to False\n",
    "blight_df['detriot_resident'] = 0\n",
    "# Fill all \"NaN\" with \"unknown\"\n",
    "blight_df['mailing_address_city'].fillna(value=\"unknown\", inplace=True)\n",
    "# Convert all characters to lower case\n",
    "blight_df.loc[:, 'mailing_address_city'] = blight_df['mailing_address_city'].str.lower()\n",
    "# Removes all punctuation\n",
    "blight_df.loc[:, 'mailing_address_city'] = blight_df['mailing_address_city'].str.extract('(^[a-z ]+)')\n",
    "# Refill all \"NaN\" with \"unknown\"\n",
    "blight_df['mailing_address_city'].fillna(value=\"unknown\", inplace=True)\n",
    "# Finds all strings that start with \"det\" and set \"detriot_resident\" to True for these values\n",
    "blight_df.loc[blight_df['mailing_address_city'].str.contains('(^det)'), 'detriot_resident'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### michigan_resident\n",
    "First fill \"NaN\" values with \"un\", then convert all the characters to lower case, and removes all punctuation. Finally use regular expressions to find all strings that start with \"mi\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aperez\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "  \n",
      "C:\\Users\\aperez\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: UserWarning: This pattern has match groups. To actually get the groups, use str.extract.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "# Creates 'michigan_resident' feature, setting all values to False\n",
    "blight_df['michigan_resident'] = 0\n",
    "# Fill all \"NaN\" with \"unknown\"\n",
    "blight_df['mailing_address_state'].fillna(value=\"un\", inplace=True)\n",
    "# Convert all characters to lower case\n",
    "blight_df.loc[:, 'mailing_address_state'] = blight_df['mailing_address_state'].str.lower()\n",
    "# Removes all punctuation\n",
    "blight_df.loc[:, 'mailing_address_state'] = blight_df['mailing_address_state'].str.extract('(^[a-z ]+)')\n",
    "# Refill all \"NaN\" with \"unknown\"\n",
    "blight_df['mailing_address_state'].fillna(value=\"un\", inplace=True)\n",
    "# Finds all strings that start with \"det\" and set \"detriot_resident\" to True for these values\n",
    "blight_df.loc[blight_df['mailing_address_state'].str.contains('(^mi)'), 'michigan_resident'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### us_resident\n",
    "First set to True for all observations that have a known state, then False for all observations that have a non-null value in country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the 'us_resident' feature\n",
    "blight_df['us_resident'] = 0 \n",
    "# Sets 'us_resident' to True if the 'mailing_address_state' is not unknown\n",
    "blight_df.loc[blight_df['mailing_address_state'] != 'un', 'us_resident'] = 1\n",
    "# Sets 'us_resident' to False if the 'mailing_address_country' is not unknown\n",
    "blight_df.loc[~blight_df['mailing_address_country'].isnull(), 'us_resident'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### violation_date\n",
    "Convert 'violation_date' to proper date format and extract the numeric month.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'violation_date' to proper date format  \n",
    "blight_df['violation_date'] = pd.DatetimeIndex(blight_df['violation_date']).date\n",
    "# Extract the numeric month. Save this as 'violation_month'\n",
    "blight_df['violation_month'] = pd.DatetimeIndex(blight_df['violation_date']).month"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### days_difference\n",
    "Convert 'hearing_date' to proper date format and subtract 'violation_date' from 'hearing_date'. Convert this difference to days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'hearing_date' to proper date format\n",
    "blight_df['hearing_date'] = pd.DatetimeIndex(blight_df['hearing_date']).date\n",
    "# Subtract 'violation_date' from 'hearing_date'. Convert this difference to days.\n",
    "blight_df['days_difference'] = (blight_df['hearing_date'] - blight_df['violation_date'])/np.timedelta64(1, 'D')\n",
    "# Remove observations where 'days_difference' is negative (This is dirty data)\n",
    "blight_df = blight_df.loc[(blight_df['days_difference'] > 0), :]\n",
    "# Remove observations where 'days_difference' is negative (This is dirty data)\n",
    "blight_df = blight_df.loc[(blight_df['days_difference'] < 365), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hearing_weekday\n",
    "Gets the name of the day of the week the 'hearing_date' is on and stores this as 'hearing_weekday'. Also does 1hot encoding on this new feature to generate five features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the name of the day of the week the hearing date is on\n",
    "blight_df['hearing_weekday'] = pd.DatetimeIndex(blight_df['hearing_date']).weekday_name\n",
    "# Converts weekdays to lowercase\n",
    "blight_df['hearing_weekday'] = blight_df['hearing_weekday'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1hot caterogical encoding for \"disposition_clean\"\n",
    "blight_weekday = blight_df['hearing_weekday']\n",
    "cat_encoder_weekday = CategoricalEncoder()\n",
    "blight_weekday_reshaped = blight_weekday.values.reshape(-1, 1)\n",
    "blight_weekday_1hot = cat_encoder_weekday.fit_transform(blight_weekday_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds new features to blight_df\n",
    "blight_df = blight_df.reset_index().merge(pd.SparseDataFrame(blight_weekday_1hot, columns=list(cat_encoder_weekday.categories_[0]), index=blight_df.index).fillna(0).reset_index())\n",
    "# Set 'ticket_id' as index\n",
    "blight_df.set_index('ticket_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### disposition_clean\n",
    "\n",
    "First initialize all the feature to be NaNs then populate it depending on the value of disposition. Afterwards we will perform one hot categorical encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize all features to be NaNs\n",
    "blight_df['disposition_clean'] = np.NaN\n",
    "# Populate the 1 (True) values\n",
    "blight_df.loc[blight_df['disposition'] == 'Responsible (Fine Waived) by Determination', 'disposition_clean'] = 'disp_fine_waived'\n",
    "blight_df.loc[blight_df['disposition'] == 'Responsible (Fine Waived) by Admission', 'disposition_clean'] = 'disp_fine_waived'\n",
    "blight_df.loc[blight_df['disposition'] == 'Responsible (Fine Waived) by Admission', 'disposition_clean'] = 'disp_fine_waived'\n",
    "# Populate the 1 (True) values\n",
    "blight_df.loc[blight_df['disposition'] == 'Responsible by Default', 'disposition_clean'] = 'disp_default'\n",
    "blight_df.loc[blight_df['disposition'] == 'Responsible - Compl/Adj by Default', 'disposition_clean'] = 'disp_default'\n",
    "# Populate the 1 (True) values\n",
    "blight_df.loc[blight_df['disposition'] == 'Responsible by Admission', 'disposition_clean'] = 'disp_admission'\n",
    "# Populate the 1 (True) values\n",
    "blight_df.loc[blight_df['disposition'] == 'Responsible by Determination', 'disposition_clean'] = 'disp_determination'\n",
    "blight_df.loc[blight_df['disposition'] == 'Responsible - Compl/Adj by Determination', 'disposition_clean'] = 'disp_determination'\n",
    "# Remove values without any 'disposition_clean' values\n",
    "blight_df.dropna(axis=0, subset=['disposition_clean'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1hot caterogical encoding for \"disposition_clean\"\n",
    "blight_disp = blight_df['disposition_clean']\n",
    "cat_encoder_disp = CategoricalEncoder()\n",
    "blight_disp_reshaped = blight_disp.values.reshape(-1, 1)\n",
    "blight_disp_1hot = cat_encoder_disp.fit_transform(blight_disp_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds new features to blight\n",
    "blight_df = blight_df.reset_index().merge(pd.SparseDataFrame(blight_disp_1hot, columns=list(cat_encoder_disp.categories_[0]), index=blight_df.index).fillna(0).reset_index())\n",
    "# Set 'ticket_id' as index\n",
    "blight_df.set_index('ticket_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### agency_name\n",
    "First I will convert all strings to lowercase, remove all non-letter characters and replace spaces with string. Then I will use 1hot encoding to covert each value into a binary feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aperez\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Cleans up values of agency name by lowercasing all laters, removing puncation and replacing spaces with underscores\n",
    "blight_df['agency_name'] = blight_df['agency_name'].str.lower().str.extract('(^[a-z ]+)').str.replace(' ', '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1hot caterogical encoding for \"agency_name\"\n",
    "blight_agency = blight_df['agency_name']\n",
    "cat_encoder_ag = CategoricalEncoder()\n",
    "blight_agency_reshaped = blight_agency.values.reshape(-1, 1)\n",
    "blight_agency_1hot = cat_encoder_ag.fit_transform(blight_agency_reshaped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds new features to blight\n",
    "blight_df = blight_df.reset_index().merge(pd.SparseDataFrame(blight_agency_1hot, columns=list(cat_encoder_ag.categories_[0]), index=blight_df.index).fillna(0).reset_index())\n",
    "# Set 'ticket_id' as index\n",
    "blight_df.set_index('ticket_id', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Violation Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "blight_df.dropna(axis=0, subset=['violation_description'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Use CountVectorizor to find three letter tokens, remove stop_words, \n",
    "# remove tokens that don't appear in at least 20 documents,\n",
    "# remove tokens that appear in more than 20% of the documents\n",
    "vect = CountVectorizer(min_df=20, max_df=0.2, stop_words='english', token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
    "\n",
    "# Fit and transform\n",
    "X = vect.fit_transform(blight_df['violation_description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aperez\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "lda = LatentDirichletAllocation(n_topics=10, learning_method=\"batch\", max_iter=25, random_state=random_seed)\n",
    "# We build the model and transform the data in one step\n",
    "# Computing transform takes some time,\n",
    "# and we can save time by doing both at once\n",
    "document_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lda.components_.shape: (10, 244)\n"
     ]
    }
   ],
   "source": [
    "print(\"lda.components_.shape: {}\".format(lda.components_.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each topic (a row in the components_), sort the features (ascending).\n",
    "# Invert rows with [:, ::-1] to make sorting descending\n",
    "sorting = np.argsort(lda.components_, axis=1)[:, ::-1]\n",
    "# get the feature names from the vectorizer:\n",
    "feature_names = np.array(vect.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(topics, feature_names, sorting, topics_per_chunk=6,\n",
    "                 n_words=20):\n",
    "    for i in range(0, len(topics), topics_per_chunk):\n",
    "        # for each chunk:\n",
    "        these_topics = topics[i: i + topics_per_chunk]\n",
    "        # maybe we have less than topics_per_chunk left\n",
    "        len_this_chunk = len(these_topics)\n",
    "        # print topic headers\n",
    "        print((\"topic {:<8}\" * len_this_chunk).format(*these_topics))\n",
    "        print((\"-------- {0:<5}\" * len_this_chunk).format(\"\"))\n",
    "        # print top n_words frequent words\n",
    "        for i in range(n_words):\n",
    "            try:\n",
    "                print((\"{:<14}\" * len_this_chunk).format(\n",
    "                    *feature_names[sorting[these_topics, i]]))\n",
    "            except:\n",
    "                pass\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0       topic 1       topic 2       topic 3       topic 4       \n",
      "--------      --------      --------      --------      --------      \n",
      "vehicle       premises      collection    public        rodent        \n",
      "motor         lie           hours         free          harborage     \n",
      "\n",
      "\n",
      "topic 5       topic 6       topic 7       topic 8       topic 9       \n",
      "--------      --------      --------      --------      --------      \n",
      "early         private       weeds         hazardous     rental        \n",
      "containers    dumping       growth        free          registration  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print out the 10 topics:\n",
    "print_topics(topics=range(10), feature_names=feature_names,\n",
    "                           sorting=sorting, topics_per_chunk=5, n_words=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# Convert sparse matrix to gensim corpus.\n",
    "corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "\n",
    "# Mapping from word IDs to words (To be used in LdaModel's id2word parameter)\n",
    "id_map = dict((v, k) for k, v in vect.vocabulary_.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the gensim.models.ldamodel.LdaModel constructor to estimate \n",
    "# LDA model parameters on the corpus, and save to the variable `ldamodel`\n",
    "\n",
    "# Your code here:\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, \n",
    "        id2word=id_map, passes=25, random_state=34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Cleanup\n",
    "Remove missing values that might have slipped away and some of the features that I already extracted useful information from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observations with no lat or long values\n",
    "blight_df.dropna(axis=0, subset=['violation_latitude'], inplace=True)\n",
    "# Drop useless features\n",
    "blight_df.drop(axis=1, columns=['mailing_address_city', 'mailing_address_state', 'mailing_address_country', \n",
    "                                'disposition', 'hearing_time'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "I will now look for trends in the features that can help me build a better model. First I will began by removing the validation data (tickets issued on 2017 and higher)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate out validation data\n",
    "validation_df = blight_df.loc[blight_df['violation_date'] >= datetime.date(2017, 1, 1), :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperate out the model building data\n",
    "model_df = blight_df.loc[blight_df['violation_date'] < datetime.date(2017, 1, 1), :].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive statistics of the numerical features.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fine_amount</th>\n",
       "      <th>admin_fee</th>\n",
       "      <th>state_fee</th>\n",
       "      <th>late_fee</th>\n",
       "      <th>discount_amount</th>\n",
       "      <th>judgment_amount</th>\n",
       "      <th>violation_latitude</th>\n",
       "      <th>violation_longitude</th>\n",
       "      <th>days_difference</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>221904.000000</td>\n",
       "      <td>221904.000000</td>\n",
       "      <td>221904.000000</td>\n",
       "      <td>221904.000000</td>\n",
       "      <td>221904.000000</td>\n",
       "      <td>221904.000000</td>\n",
       "      <td>221904.000000</td>\n",
       "      <td>221904.000000</td>\n",
       "      <td>221904.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>335.040959</td>\n",
       "      <td>19.682756</td>\n",
       "      <td>9.926536</td>\n",
       "      <td>4.184554</td>\n",
       "      <td>1.245045</td>\n",
       "      <td>373.274500</td>\n",
       "      <td>42.390810</td>\n",
       "      <td>-83.113271</td>\n",
       "      <td>57.859791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>605.438652</td>\n",
       "      <td>2.475922</td>\n",
       "      <td>0.857738</td>\n",
       "      <td>20.882992</td>\n",
       "      <td>8.295947</td>\n",
       "      <td>622.332047</td>\n",
       "      <td>0.037141</td>\n",
       "      <td>0.094544</td>\n",
       "      <td>43.792055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>-60.000000</td>\n",
       "      <td>-30.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>42.221239</td>\n",
       "      <td>-83.317266</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>42.365092</td>\n",
       "      <td>-83.190627</td>\n",
       "      <td>27.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>250.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>42.395772</td>\n",
       "      <td>-83.128873</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>250.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>280.000000</td>\n",
       "      <td>42.420706</td>\n",
       "      <td>-83.036507</td>\n",
       "      <td>76.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>10000.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>15538.800000</td>\n",
       "      <td>42.474003</td>\n",
       "      <td>-82.885328</td>\n",
       "      <td>364.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         fine_amount      admin_fee      state_fee       late_fee  \\\n",
       "count  221904.000000  221904.000000  221904.000000  221904.000000   \n",
       "mean      335.040959      19.682756       9.926536       4.184554   \n",
       "std       605.438652       2.475922       0.857738      20.882992   \n",
       "min         1.000000     -60.000000     -30.000000      -1.000000   \n",
       "25%       100.000000      20.000000      10.000000       0.000000   \n",
       "50%       250.000000      20.000000      10.000000       0.000000   \n",
       "75%       250.000000      20.000000      10.000000       0.000000   \n",
       "max     10000.000000      20.000000      20.000000    1000.000000   \n",
       "\n",
       "       discount_amount  judgment_amount  violation_latitude  \\\n",
       "count    221904.000000    221904.000000       221904.000000   \n",
       "mean          1.245045       373.274500           42.390810   \n",
       "std           8.295947       622.332047            0.037141   \n",
       "min           0.000000       -50.000000           42.221239   \n",
       "25%           0.000000       130.000000           42.365092   \n",
       "50%           0.000000       280.000000           42.395772   \n",
       "75%           0.000000       280.000000           42.420706   \n",
       "max        1000.000000     15538.800000           42.474003   \n",
       "\n",
       "       violation_longitude  days_difference  \n",
       "count        221904.000000    221904.000000  \n",
       "mean            -83.113271        57.859791  \n",
       "std               0.094544        43.792055  \n",
       "min             -83.317266         1.000000  \n",
       "25%             -83.190627        27.000000  \n",
       "50%             -83.128873        42.000000  \n",
       "75%             -83.036507        76.000000  \n",
       "max             -82.885328       364.000000  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Gets all the numerical features\n",
    "num_features = ['fine_amount', 'admin_fee', 'state_fee', 'late_fee', 'discount_amount', 'judgment_amount',\n",
    " 'violation_latitude', 'violation_longitude', 'days_difference']\n",
    "# A quick view of the descriptive statistics of the numerical features\n",
    "print('Descriptive statistics of the numerical features.')\n",
    "violation_df[num_features].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x19393da2cc0>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAD8CAYAAABHN8LqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1938ae8a940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "violation_df['fine_amount'].plot.hist(by='compliance', bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
